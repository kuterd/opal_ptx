{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512a3491-533e-4fee-aed5-00ec7fbafa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opal_ptx import CuModuleWrapper, kernel_transformer\n",
    "\n",
    "from torch.utils.cpp_extension import load\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f7ff35-34b4-4641-aae8-2f1dcb4b469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kernel_transformer.kernel()\n",
    "def simple_wmma_gemm(A: \"u64\", B: \"u64\", D: \"u64\", m: \"u32\", n: \"u32\", k: \"u32\"):\n",
    "    x: u32 = u32(\"%ctaid.x\") * 16\n",
    "    y: u32 = u32(\"%ctaid.y\") * 16\n",
    "\n",
    "    a: b32(8)\n",
    "    b: b32(8)\n",
    "    d: b32(8)\n",
    "\n",
    "    for i in range(8):\n",
    "        ptx.mov.b32(d[i], 0)\n",
    "    \n",
    "    KI: u32 = 0\n",
    "    while KI < k:\n",
    "        _A: u64 = A + (y * k + KI) * 2\n",
    "        _B: u64 = B + (KI * n + x) * 2\n",
    "        \n",
    "        ptx.wmma.load.a.sync.aligned._global.m16n16k16.row.f16({*a}, [_A], k)\n",
    "        ptx.wmma.load.b.sync.aligned._global.m16n16k16.row.f16({*b}, [_B], n)\n",
    "    \n",
    "        ptx.wmma.mma.sync.aligned.m16n16k16.row.row.f32.f32({*d}, {*a}, {*b}, {*d})\n",
    "        KI += 16\n",
    "\n",
    "    _D: u64 = D + (y * n + x) * 4\n",
    "    ptx.wmma.store.d.sync.aligned.m16n16k16._global.row.f32([_D], {*d}, m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b68c7679-5c51-425c-a45d-c6d5112d2ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2263, 0.0316, 0.0526,  ..., 0.9868, 0.8169, 0.5220],\n",
      "        [0.9502, 0.2150, 0.6509,  ..., 0.4800, 0.8105, 0.3823],\n",
      "        [0.7480, 0.5503, 0.3638,  ..., 0.0263, 0.2659, 0.4893],\n",
      "        ...,\n",
      "        [0.0878, 0.2206, 0.6890,  ..., 0.4414, 0.4863, 0.3894],\n",
      "        [0.7188, 0.8145, 0.6309,  ..., 0.2413, 0.3589, 0.5679],\n",
      "        [0.6646, 0.8462, 0.4480,  ..., 0.6274, 0.3765, 0.9536]],\n",
      "       device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "kernel_builder = kernel_transformer.KernelBuilder()\n",
    "simple_wmma_gemm(kernel_builder)\n",
    "kernel_code = kernel_builder.generate()\n",
    "\n",
    "M = 32\n",
    "N = 32\n",
    "K = 32\n",
    "\n",
    "#A = torch.eye(K, dtype=torch.float16, device=\"cuda\")\n",
    "#A = torch.triu(torch.full((K, K), 3, dtype=torch.float16, device=\"cuda\"))\n",
    "A = torch.rand((M, K), dtype=torch.float16, device=\"cuda\")\n",
    "B = torch.eye(K, dtype=torch.float16, device=\"cuda\")\n",
    "\n",
    "D = torch.zeros((M, N), dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "check = A @ B\n",
    "\n",
    "wrapper = CuModuleWrapper()\n",
    "wrapper.load_ptx_code(kernel_code)\n",
    "\n",
    "wrapper.launch_kernel(\"simple_wmma_gemm\", ((M // 16), (N // 16), 1), (32, 1, 1), (A.data_ptr(), B.data_ptr(), D.data_ptr(), M, N, K))\n",
    "\n",
    "print(D)\n",
    "print((D - check).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0689c4-93bb-44d7-b1a0-f112184d9b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
